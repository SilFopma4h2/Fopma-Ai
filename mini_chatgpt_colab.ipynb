{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-ChatGPT in Google Colab\n",
    "\n",
    "This notebook implements a mini version of ChatGPT using The Pile dataset from Hugging Face.\n",
    "\n",
    "## Features:\n",
    "- Streams a subset of The Pile dataset\n",
    "- Uses GPT-2 tokenizer for text processing\n",
    "- Implements a small GPT model (2-4 transformer layers)\n",
    "- Trains with AdamW optimizer\n",
    "- Provides a simple chat interface\n",
    "\n",
    "## Requirements:\n",
    "- Google Colab or local Jupyter environment\n",
    "- GPU recommended for training\n",
    "\n",
    "**Note**: This is an educational implementation optimized for Colab's resource constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch accelerate tqdm numpy\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Stream The Pile Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a subset of The Pile dataset\n",
    "# We'll use a smaller subset for Colab compatibility\n",
    "print(\"Loading The Pile dataset...\")\n",
    "try:\n",
    "    # Load the train split with streaming=True for memory efficiency\n",
    "    dataset = load_dataset(\"EleutherAI/pile\", streaming=True, split=\"train\")\n",
    "    print(\"Successfully loaded The Pile dataset\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading The Pile: {e}\")\n",
    "    print(\"Falling back to a smaller text dataset...\")\n",
    "    # Fallback to a smaller dataset if The Pile is not accessible\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", streaming=True, split=\"train\")\n",
    "    print(\"Using WikiText-2 as fallback dataset\")\n",
    "\n",
    "# Take a small sample for demonstration (adjust as needed for your Colab session)\n",
    "sample_size = 5000  # Number of texts to use for training\n",
    "print(f\"Using {sample_size} samples for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.texts = []\n",
    "        \n",
    "        print(\"Tokenizing texts...\")\n",
    "        for text in tqdm(texts, desc=\"Processing texts\"):\n",
    "            if isinstance(text, dict):\n",
    "                # Handle different dataset formats\n",
    "                text_content = text.get('text', '') or text.get('content', '')\n",
    "            else:\n",
    "                text_content = str(text)\n",
    "            \n",
    "            # Skip very short texts\n",
    "            if len(text_content.strip()) < 50:\n",
    "                continue\n",
    "                \n",
    "            # Tokenize and truncate\n",
    "            tokens = tokenizer.encode(text_content, max_length=max_length, truncation=True)\n",
    "            if len(tokens) > 10:  # Only keep texts with reasonable length\n",
    "                self.texts.append(tokens)\n",
    "        \n",
    "        print(f\"Processed {len(self.texts)} valid texts\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts[idx]\n",
    "        \n",
    "        # For language modeling, input and target are the same, shifted by one\n",
    "        if len(tokens) > 1:\n",
    "            input_ids = tokens[:-1]\n",
    "            target_ids = tokens[1:]\n",
    "        else:\n",
    "            input_ids = tokens\n",
    "            target_ids = tokens\n",
    "        \n",
    "        # Pad to max_length\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * (self.max_length - 1 - len(input_ids))\n",
    "        target_ids = target_ids + [tokenizer.pad_token_id] * (self.max_length - 1 - len(target_ids))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids[:self.max_length-1], dtype=torch.long),\n",
    "            'target_ids': torch.tensor(target_ids[:self.max_length-1], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Collect sample texts\n",
    "print(\"Collecting sample texts from dataset...\")\n",
    "texts = []\n",
    "for i, example in enumerate(dataset):\n",
    "    if i >= sample_size:\n",
    "        break\n",
    "    texts.append(example)\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"Collected {i + 1} texts\")\n",
    "\n",
    "print(f\"Total texts collected: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "max_seq_length = 128  # Smaller sequence length for Colab\n",
    "batch_size = 8  # Small batch size for memory efficiency\n",
    "\n",
    "train_dataset = TextDataset(texts, tokenizer, max_length=max_seq_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Number of batches: {len(train_dataloader)}\")\n",
    "\n",
    "# Show a sample batch\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"\\nSample batch shape:\")\n",
    "print(f\"Input IDs: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"Target IDs: {sample_batch['target_ids'].shape}\")\n",
    "\n",
    "# Decode a sample to verify\n",
    "sample_text = tokenizer.decode(sample_batch['input_ids'][0], skip_special_tokens=True)\n",
    "print(f\"\\nSample text preview: {sample_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mini GPT Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        \n",
    "        output = self.W_o(attention_output)\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attention_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, num_heads=8, num_layers=4, d_ff=1024, max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def create_causal_mask(self, seq_len):\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "        return mask.view(1, 1, seq_len, seq_len)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        # Create position indices\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_len)\n",
    "        \n",
    "        # Embeddings\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        position_embeds = self.position_embedding(position_ids)\n",
    "        x = self.dropout(token_embeds + position_embeds)\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = self.create_causal_mask(seq_len).to(input_ids.device)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, mask)\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Model configuration\n",
    "model_config = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'd_model': 256,  # Hidden size\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 4,  # Number of transformer layers\n",
    "    'd_ff': 1024,  # Feed-forward dimension\n",
    "    'max_seq_len': max_seq_length,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = MiniGPT(**model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model configuration: {model_config}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration and Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 2  # Limited for Colab\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 100\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    return math.exp(loss)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "total_loss = 0\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Calculate loss\n",
    "        # Reshape for cross entropy: (batch_size * seq_len, vocab_size)\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        targets_flat = target_ids.view(-1)\n",
    "        \n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        epoch_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        step += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        perplexity = calculate_perplexity(loss.item())\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'ppl': f'{perplexity:.2f}',\n",
    "            'lr': f'{current_lr:.2e}'\n",
    "        })\n",
    "        \n",
    "        # Log every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            avg_loss = total_loss / step\n",
    "            print(f\"\\nStep {step}: avg_loss = {avg_loss:.4f}, perplexity = {calculate_perplexity(avg_loss):.2f}\")\n",
    "    \n",
    "    # End of epoch summary\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    epoch_perplexity = calculate_perplexity(avg_epoch_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} completed:\")\n",
    "    print(f\"Average loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"Perplexity: {epoch_perplexity:.2f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "final_avg_loss = total_loss / step\n",
    "final_perplexity = calculate_perplexity(final_avg_loss)\n",
    "print(f\"Final average loss: {final_avg_loss:.4f}\")\n",
    "print(f\"Final perplexity: {final_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chat Interface Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.8, top_k=50, top_p=0.95):\n",
    "    \"\"\"\n",
    "    Generate text using the trained model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained MiniGPT model\n",
    "        tokenizer: GPT-2 tokenizer\n",
    "        prompt: Input text prompt\n",
    "        max_length: Maximum length of generated text\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_k: Top-k sampling\n",
    "        top_p: Top-p (nucleus) sampling\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get model predictions\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[:, -1, :]  # Get logits for the last token\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k > 0:\n",
    "                top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "                logits_filtered = torch.full_like(logits, float('-inf'))\n",
    "                logits_filtered.scatter_(1, top_k_indices, top_k_logits)\n",
    "                logits = logits_filtered\n",
    "            \n",
    "            # Apply top-p filtering\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                # Remove tokens with cumulative probability above the threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                \n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample from the filtered distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Add the new token to input_ids\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "            \n",
    "            # Check if we've hit the EOS token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            # Prevent sequences from getting too long for memory\n",
    "            if input_ids.shape[1] > model.max_seq_len:\n",
    "                break\n",
    "    \n",
    "    # Decode the generated sequence\n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def chat():\n",
    "    \"\"\"\n",
    "    Simple chat interface for interacting with the model\n",
    "    \"\"\"\n",
    "    print(\"Mini-ChatGPT is ready! Type 'quit' to exit.\")\n",
    "    print(\"Note: This is a simple model trained on limited data, so responses may be basic.\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"\\nYou: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Generate response\n",
    "            print(\"AI: \", end=\"\", flush=True)\n",
    "            response = generate_text(\n",
    "                model, \n",
    "                tokenizer, \n",
    "                user_input, \n",
    "                max_length=50,  # Keep responses short\n",
    "                temperature=0.8,\n",
    "                top_k=50,\n",
    "                top_p=0.95\n",
    "            )\n",
    "            \n",
    "            # Extract just the new part (after the input)\n",
    "            if response.startswith(user_input):\n",
    "                ai_response = response[len(user_input):].strip()\n",
    "            else:\n",
    "                ai_response = response.strip()\n",
    "            \n",
    "            print(ai_response if ai_response else \"[Model generated empty response]\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "# Test the generation function\n",
    "print(\"Testing text generation...\")\n",
    "test_prompts = [\n",
    "    \"The weather today is\",\n",
    "    \"Artificial intelligence is\",\n",
    "    \"In the future, we will\",\n",
    "    \"Programming is\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=30, temperature=0.8)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Generated: '{generated}'\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Chat Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the chat interface\n",
    "# Uncomment the line below to start chatting with your mini-GPT!\n",
    "# chat()\n",
    "\n",
    "# For demonstration purposes, let's show some example interactions\n",
    "print(\"Example interactions with Mini-ChatGPT:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "example_prompts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Tell me a story about\",\n",
    "    \"The benefits of Python programming are\",\n",
    "    \"Explain what a neural network\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(example_prompts, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"User: {prompt}\")\n",
    "    \n",
    "    response = generate_text(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        prompt, \n",
    "        max_length=40,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    \n",
    "    # Extract the generated part\n",
    "    if response.startswith(prompt):\n",
    "        ai_response = response[len(prompt):].strip()\n",
    "    else:\n",
    "        ai_response = response.strip()\n",
    "    \n",
    "    print(f\"AI: {prompt}{ai_response}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "print(\"Model Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Memory usage\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated(device) / 1024**2  # MB\n",
    "    memory_reserved = torch.cuda.memory_reserved(device) / 1024**2   # MB\n",
    "    print(f\"GPU Memory Allocated: {memory_allocated:.1f} MB\")\n",
    "    print(f\"GPU Memory Reserved: {memory_reserved:.1f} MB\")\n",
    "\n",
    "# Model size analysis\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"- Vocabulary Size: {model_config['vocab_size']:,}\")\n",
    "print(f\"- Hidden Dimensions: {model_config['d_model']}\")\n",
    "print(f\"- Number of Layers: {model_config['num_layers']}\")\n",
    "print(f\"- Number of Attention Heads: {model_config['num_heads']}\")\n",
    "print(f\"- Feed-Forward Dimension: {model_config['d_ff']}\")\n",
    "print(f\"- Maximum Sequence Length: {model_config['max_seq_len']}\")\n",
    "\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "print(f\"- Dataset Size: {len(train_dataset):,} samples\")\n",
    "print(f\"- Batch Size: {batch_size}\")\n",
    "print(f\"- Training Steps: {step:,}\")\n",
    "print(f\"- Final Loss: {final_avg_loss:.4f}\")\n",
    "print(f\"- Final Perplexity: {final_perplexity:.2f}\")\n",
    "\n",
    "# Save model checkpoint (optional)\n",
    "print(f\"\\nSaving model checkpoint...\")\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'model_config': model_config,\n",
    "    'training_loss': final_avg_loss,\n",
    "    'training_steps': step,\n",
    "    'epochs': num_epochs\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, '/content/mini_chatgpt_checkpoint.pt')\n",
    "print(\"Model saved as 'mini_chatgpt_checkpoint.pt'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Mini-ChatGPT implementation completed!\")\n",
    "print(\"\\nTo chat with your model, uncomment and run the chat() function above.\")\n",
    "print(\"\\nNote: This is a basic implementation for educational purposes.\")\n",
    "print(\"For production use, consider:\")\n",
    "print(\"- More training data and longer training\")\n",
    "print(\"- Larger model architecture\")\n",
    "print(\"- Advanced techniques like RLHF\")\n",
    "print(\"- Better evaluation metrics\")\n",
    "print(\"- Safety and bias considerations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Usage Instructions and Next Steps\n",
    "\n",
    "### How to Use This Notebook:\n",
    "\n",
    "1. **Run all cells sequentially** - The notebook is designed to be executed from top to bottom\n",
    "2. **Monitor training** - Watch the loss and perplexity metrics during training\n",
    "3. **Test generation** - The notebook includes automatic testing of text generation\n",
    "4. **Interactive chat** - Uncomment the `chat()` function call to start an interactive session\n",
    "\n",
    "### Customization Options:\n",
    "\n",
    "- **Model size**: Adjust `d_model`, `num_layers`, `num_heads` in the model configuration\n",
    "- **Training data**: Modify `sample_size` to use more/less data\n",
    "- **Sequence length**: Change `max_seq_length` for longer/shorter contexts\n",
    "- **Training duration**: Adjust `num_epochs` and learning parameters\n",
    "- **Generation parameters**: Modify `temperature`, `top_k`, `top_p` for different text styles\n",
    "\n",
    "### Limitations and Considerations:\n",
    "\n",
    "- This is a simplified implementation for educational purposes\n",
    "- The model is small and trained on limited data\n",
    "- Responses may not be coherent or factually accurate\n",
    "- No safety filtering or bias mitigation is implemented\n",
    "- GPU memory constraints limit model size in Colab\n",
    "\n",
    "### Possible Improvements:\n",
    "\n",
    "- Use larger models with more parameters\n",
    "- Train on more diverse and larger datasets\n",
    "- Implement advanced training techniques (gradient accumulation, mixed precision)\n",
    "- Add instruction following capabilities\n",
    "- Implement safety and content filtering\n",
    "- Add evaluation metrics (BLEU, ROUGE, etc.)\n",
    "- Create a web interface for easier interaction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}